{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Understanding - Programming Assignment 3\n",
    "\n",
    "    Aryan Tiwari B20AI056\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# import gradio as gr\n",
    "import wandb as wb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "**Goal:** \n",
    "\n",
    "    The task is to classify the audio samples into Real and Fake\n",
    "\n",
    "**Tasks:** \n",
    "\n",
    "    —- Use the SSL W2V model trained for LA and DF tracks of the ASVSpoof dataset.\n",
    "\n",
    "    —- Download the custom dataset from here. Report the AUC and EER on this dataset. \n",
    "    \n",
    "    —- Analyze the performance of the model.\n",
    "    \n",
    "    —- Finetune the model on FOR dataset. \n",
    "    \n",
    "    —- Report the performance using AUC and EER on For dataset. \n",
    "    \n",
    "    —- Use the model trained on the FOR dataset to evaluate the custom dataset. Report the EER and AUC\n",
    "    \n",
    "    —- Comment on the change in performance, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TakHemlata/SSL_Anti-spoofing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import fairseq\n",
    "\n",
    "\n",
    "############################\n",
    "## FOR fine-tuned SSL MODEL\n",
    "############################\n",
    "\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(SSLModel, self).__init__()\n",
    "        \n",
    "        cp_path = 'xlsr2_300m.pt'   # Change the pre-trained XLSR model path. \n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "        self.model = model[0]\n",
    "        self.device=device\n",
    "        self.out_dim = 1024\n",
    "        return\n",
    "\n",
    "    def extract_feat(self, input_data):\n",
    "        \n",
    "        # put the model to GPU if it not there\n",
    "        if next(self.model.parameters()).device != input_data.device \\\n",
    "           or next(self.model.parameters()).dtype != input_data.dtype:\n",
    "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
    "            self.model.train()\n",
    "\n",
    "        \n",
    "        if True:\n",
    "            # input should be in shape (batch, length)\n",
    "            if input_data.ndim == 3:\n",
    "                input_tmp = input_data[:, :, 0]\n",
    "            else:\n",
    "                input_tmp = input_data\n",
    "                \n",
    "            # [batch, length, dim]\n",
    "            emb = self.model(input_tmp, mask=False, features_only=True)['x']\n",
    "        return emb\n",
    "\n",
    "\n",
    "#---------AASIST back-end------------------------#\n",
    "''' Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu and Nicholas Evans. \n",
    "    AASIST: Audio Anti-Spoofing Using Integrated Spectro-Temporal Graph Attention Networks. \n",
    "    In Proc. ICASSP 2022, pp: 6367--6371.'''\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_weight = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x   :(#bs, #node, #dim)\n",
    "        '''\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x)\n",
    "\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map(self, x):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "        att_map = torch.matmul(att_map, self.att_weight)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class HtrgGraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_type1 = nn.Linear(in_dim, in_dim)\n",
    "        self.proj_type2 = nn.Linear(in_dim, in_dim)\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_projM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.att_weight11 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight22 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight12 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weightM = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x1, x2, master=None):\n",
    "        '''\n",
    "        x1  :(#bs, #node, #dim)\n",
    "        x2  :(#bs, #node, #dim)\n",
    "        '''\n",
    "        #print('x1',x1.shape)\n",
    "        #print('x2',x2.shape)\n",
    "        num_type1 = x1.size(1)\n",
    "        num_type2 = x2.size(1)\n",
    "        #print('num_type1',num_type1)\n",
    "        #print('num_type2',num_type2)\n",
    "        x1 = self.proj_type1(x1)\n",
    "        #print('proj_type1',x1.shape)\n",
    "        x2 = self.proj_type2(x2)\n",
    "        #print('proj_type2',x2.shape)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #print('Concat x1 and x2',x.shape)\n",
    "        \n",
    "        if master is None:\n",
    "            master = torch.mean(x, dim=1, keepdim=True)\n",
    "            #print('master',master.shape)\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x, num_type1, num_type2)\n",
    "        #print('master',master.shape)\n",
    "        # directional edge for master node\n",
    "        master = self._update_master(x, master)\n",
    "        #print('master',master.shape)\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "        #print('proj x',x.shape)\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x1 = x.narrow(1, 0, num_type1)\n",
    "        #print('x1',x1.shape)\n",
    "        x2 = x.narrow(1, num_type1, num_type2)\n",
    "        #print('x2',x2.shape)\n",
    "        return x1, x2, master\n",
    "\n",
    "    def _update_master(self, x, master):\n",
    "\n",
    "        att_map = self._derive_att_map_master(x, master)\n",
    "        master = self._project_master(x, master, att_map)\n",
    "\n",
    "        return master\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map_master(self, x, master):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = x * master\n",
    "        att_map = torch.tanh(self.att_projM(att_map))\n",
    "\n",
    "        att_map = torch.matmul(att_map, self.att_weightM)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _derive_att_map(self, x, num_type1, num_type2):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "\n",
    "        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n",
    "\n",
    "        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n",
    "        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n",
    "        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n",
    "        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n",
    "\n",
    "        att_map = att_board\n",
    "\n",
    "        \n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _project_master(self, x, master, att_map):\n",
    "\n",
    "        x1 = self.proj_with_attM(torch.matmul(\n",
    "            att_map.squeeze(-1).unsqueeze(1), x))\n",
    "        x2 = self.proj_without_attM(master)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "    def forward(self, h):\n",
    "        Z = self.drop(h)\n",
    "        weights = self.proj(Z)\n",
    "        scores = self.sigmoid(weights)\n",
    "        new_h = self.top_k_graph(scores, h, self.k)\n",
    "\n",
    "        return new_h\n",
    "\n",
    "    def top_k_graph(self, scores, h, k):\n",
    "        \"\"\"\n",
    "        args\n",
    "        =====\n",
    "        scores: attention-based weights (#bs, #node, 1)\n",
    "        h: graph data (#bs, #node, #dim)\n",
    "        k: ratio of remaining nodes, (float)\n",
    "        returns\n",
    "        =====\n",
    "        h: graph pool applied data (#bs, #node', #dim)\n",
    "        \"\"\"\n",
    "        _, n_nodes, n_feat = h.size()\n",
    "        n_nodes = max(int(n_nodes * k), 1)\n",
    "        _, idx = torch.topk(scores, n_nodes, dim=1)\n",
    "        idx = idx.expand(-1, -1, n_feat)\n",
    "\n",
    "        h = h * scores\n",
    "        h = torch.gather(h, 1, idx)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, nb_filts, first=False):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "\n",
    "        if not self.first:\n",
    "            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n",
    "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(1, 1),\n",
    "                               stride=1)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
    "        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(0, 1),\n",
    "                               stride=1)\n",
    "\n",
    "        if nb_filts[0] != nb_filts[1]:\n",
    "            self.downsample = True\n",
    "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                                             out_channels=nb_filts[1],\n",
    "                                             padding=(0, 1),\n",
    "                                             kernel_size=(1, 3),\n",
    "                                             stride=1)\n",
    "\n",
    "        else:\n",
    "            self.downsample = False\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if not self.first:\n",
    "            out = self.bn1(x)\n",
    "            out = self.selu(out)\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        #print('out',out.shape)\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        #print('aft conv1 out',out.shape)\n",
    "        out = self.bn2(out)\n",
    "        out = self.selu(out)\n",
    "        # print('out',out.shape)\n",
    "        out = self.conv2(out)\n",
    "        #print('conv2 out',out.shape)\n",
    "        \n",
    "        if self.downsample:\n",
    "            identity = self.conv_downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        #out = self.mp(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # AASIST parameters\n",
    "        filts = [128, [1, 32], [32, 32], [32, 64], [64, 64]]\n",
    "        gat_dims = [64, 32]\n",
    "        pool_ratios = [0.5, 0.5, 0.5, 0.5]\n",
    "        temperatures =  [2.0, 2.0, 100.0, 100.0]\n",
    "\n",
    "\n",
    "        ####\n",
    "        # create network wav2vec 2.0\n",
    "        ####\n",
    "        self.ssl_model = SSLModel(self.device)\n",
    "        self.LL = nn.Linear(self.ssl_model.out_dim, 128)\n",
    "\n",
    "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
    "        self.first_bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.drop = nn.Dropout(0.5, inplace=True)\n",
    "        self.drop_way = nn.Dropout(0.2, inplace=True)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        # RawNet2 encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[2])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[3])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])))\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(1,1)),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 64, kernel_size=(1,1)),\n",
    "            \n",
    "        )\n",
    "        # position encoding\n",
    "        self.pos_S = nn.Parameter(torch.randn(1, 42, filts[-1][-1]))\n",
    "        \n",
    "        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "        \n",
    "        # Graph module\n",
    "        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[0])\n",
    "        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[1])\n",
    "        # HS-GAL layer \n",
    "        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "\n",
    "        # Graph pooling layers\n",
    "        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n",
    "        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n",
    "        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "\n",
    "        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        \n",
    "        self.out_layer = nn.Linear(5 * gat_dims[1], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #-------pre-trained Wav2vec model fine tunning ------------------------##\n",
    "        x_ssl_feat = self.ssl_model.extract_feat(x.squeeze(-1))\n",
    "        x = self.LL(x_ssl_feat) #(bs,frame_number,feat_out_dim)\n",
    "        \n",
    "        # post-processing on front-end features\n",
    "        x = x.transpose(1, 2)   #(bs,feat_out_dim,frame_number)\n",
    "        x = x.unsqueeze(dim=1) # add channel \n",
    "        x = F.max_pool2d(x, (3, 3))\n",
    "        x = self.first_bn(x)\n",
    "        x = self.selu(x)\n",
    "\n",
    "        # RawNet2-based encoder\n",
    "        x = self.encoder(x)\n",
    "        x = self.first_bn1(x)\n",
    "        x = self.selu(x)\n",
    "        \n",
    "        w = self.attention(x)\n",
    "        \n",
    "        #------------SA for spectral feature-------------#\n",
    "        w1 = F.softmax(w,dim=-1)\n",
    "        m = torch.sum(x * w1, dim=-1)\n",
    "        e_S = m.transpose(1, 2) + self.pos_S \n",
    "        \n",
    "        # graph module layer\n",
    "        gat_S = self.GAT_layer_S(e_S)\n",
    "        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n",
    "        \n",
    "        #------------SA for temporal feature-------------#\n",
    "        w2 = F.softmax(w,dim=-2)\n",
    "        m1 = torch.sum(x * w2, dim=-2)\n",
    "     \n",
    "        e_T = m1.transpose(1, 2)\n",
    "       \n",
    "        # graph module layer\n",
    "        gat_T = self.GAT_layer_T(e_T)\n",
    "        out_T = self.pool_T(gat_T)\n",
    "        \n",
    "        # learnable master node\n",
    "        master1 = self.master1.expand(x.size(0), -1, -1)\n",
    "        master2 = self.master2.expand(x.size(0), -1, -1)\n",
    "\n",
    "        # inference 1\n",
    "        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n",
    "            out_T, out_S, master=self.master1)\n",
    "\n",
    "        out_S1 = self.pool_hS1(out_S1)\n",
    "        out_T1 = self.pool_hT1(out_T1)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n",
    "            out_T1, out_S1, master=master1)\n",
    "        out_T1 = out_T1 + out_T_aug\n",
    "        out_S1 = out_S1 + out_S_aug\n",
    "        master1 = master1 + master_aug\n",
    "\n",
    "        # inference 2\n",
    "        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n",
    "            out_T, out_S, master=self.master2)\n",
    "        out_S2 = self.pool_hS2(out_S2)\n",
    "        out_T2 = self.pool_hT2(out_T2)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n",
    "            out_T2, out_S2, master=master2)\n",
    "        out_T2 = out_T2 + out_T_aug\n",
    "        out_S2 = out_S2 + out_S_aug\n",
    "        master2 = master2 + master_aug\n",
    "\n",
    "        out_T1 = self.drop_way(out_T1)\n",
    "        out_T2 = self.drop_way(out_T2)\n",
    "        out_S1 = self.drop_way(out_S1)\n",
    "        out_S2 = self.drop_way(out_S2)\n",
    "        master1 = self.drop_way(master1)\n",
    "        master2 = self.drop_way(master2)\n",
    "\n",
    "        out_T = torch.max(out_T1, out_T2)\n",
    "        out_S = torch.max(out_S1, out_S2)\n",
    "        master = torch.max(master1, master2)\n",
    "\n",
    "        # Readout operation\n",
    "        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n",
    "        T_avg = torch.mean(out_T, dim=1)\n",
    "\n",
    "        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n",
    "        S_avg = torch.mean(out_S, dim=1)\n",
    "        \n",
    "        last_hidden = torch.cat(\n",
    "            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n",
    "        \n",
    "        last_hidden = self.drop(last_hidden)\n",
    "        output = self.out_layer(last_hidden)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Configuration Management\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.experimentSaveName = 'customSubsetEval1'\n",
    "        self.rootDirectoryPathRishabhSubset = '../data/Dataset_Speech_Assignment' \n",
    "        self.batchSize = 8\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.randomSeed = 42\n",
    "        torch.manual_seed(self.randomSeed)\n",
    "\n",
    "# Model Loading\n",
    "def loadModel(modelClass, stateDictPath, args):\n",
    "    model = modelClass(args, 'cpu')\n",
    "    stateDict = torch.load(stateDictPath, map_location='cpu')\n",
    "    for key in list(stateDict.keys()):\n",
    "        stateDict[key.replace('module.', '')] = stateDict.pop(key)\n",
    "    model.load_state_dict(stateDict)\n",
    "    return model\n",
    "\n",
    "# Dataset and DataLoader Setup\n",
    "def pad(x, maxLength=64600):\n",
    "    xLength = x.shape[0]\n",
    "    if xLength >= maxLength:\n",
    "        return x[:maxLength]\n",
    "    numRepeats = int(maxLength / xLength) + 1\n",
    "    paddedX = np.tile(x, (1, numRepeats))[:, :maxLength][0]\n",
    "    return paddedX\n",
    "\n",
    "def loaderRishabhSubset(samplePath):\n",
    "    cut = 64600\n",
    "    X, fs = librosa.load(samplePath, sr=16000)\n",
    "    XPad = pad(X, cut)\n",
    "    xInput = torch.Tensor(XPad)\n",
    "    return xInput\n",
    "\n",
    "def setupDataLoader(rootDirectoryPath, loaderFunc, extensions, batchSize):\n",
    "    dataset = DatasetFolder(rootDirectoryPath, loader=loaderFunc, extensions=extensions)\n",
    "    loader = DataLoader(dataset, batch_size=batchSize, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Evaluation Utilities\n",
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "    target_scores = np.array(target_scores)\n",
    "    nontarget_scores = np.array(nontarget_scores)\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))\n",
    "    return frr, far, thresholds\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "def compute_eer_auc(target_scores, nontarget_scores):\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    auc = np.trapz(1 - frr, far)\n",
    "    return eer, auc, thresholds[min_index]\n",
    "\n",
    "# Evaluation Script\n",
    "def evalScript(model, loader, device, experiment_save_name, savePath='B20AI056_eval', printLogs=True, saveLogs=True, saveFigs=True):\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        scores = []\n",
    "        truths = []\n",
    "        for xs, labels in tqdm(loader):\n",
    "            xs, labels = xs.to(device), labels.to(device)\n",
    "            outputs = model(xs)\n",
    "            scores.extend((outputs[:, 1]).data.cpu().numpy().ravel().tolist())\n",
    "            truths.extend(labels.tolist())\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(np.array(truths), np.array(scores), pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        thresh = interp1d(fpr, thresholds)(eer)\n",
    "\n",
    "        if printLogs:\n",
    "            print('FINAL SCORES BELOW!!!!!!!!!!')\n",
    "            print('eer:', eer, '        auc:', auc,  \"    thresh:\", thresh)\n",
    "            print('FINAL SCORES ABOVE!!!!!!!!!!')\n",
    "            print()\n",
    "        if saveLogs:\n",
    "            with open(os.path.join(savePath, experiment_save_name + '.json'), 'w') as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        'eer': float(f'{eer}'),\n",
    "                        'auc': float(f'{auc}'),\n",
    "                        'thresh': float(f'{thresh}'),\n",
    "                    }, f, indent=4\n",
    "                )\n",
    "            \n",
    "        if saveFigs:    \n",
    "            # calculate the precision recall and F1-Score\n",
    "            precision, recall, _ = metrics.precision_recall_curve(truths, scores)\n",
    "            f1 = metrics.f1_score(truths, scores)\n",
    "            plt.figure()\n",
    "            plt.plot(recall, precision, marker='.')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title('Precision Recall Curve')\n",
    "            plt.grid()\n",
    "            plt.savefig(os.path.join(savePath, experiment_save_name + '_precision_recall_curve.png'))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    return eer, auc\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    config = Config()\n",
    "    model = loadModel(Model, '../models/BestLAModelForDF.pth', None)  # Adjust args if needed\n",
    "    loader = setupDataLoader(config.rootDirectoryPathRishabhSubset, loaderRishabhSubset, ('wav', 'mp3'), config.batchSize)\n",
    "    evalScript(model, loader, config.device, config.experimentSaveName, printLogs=False, saveLogs=False, saveFigs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import os\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.checkpointSaveDir = 'checkpoint_save_dir'\n",
    "        self.experimentSaveName = 'trainEval2Final'\n",
    "        self.rootDirectoryPathRishabhSubset = '../data/Dataset_Speech_Assignment' \n",
    "        self.rootDirectoryPathTesting = '../data/Dataset_Speech_Assignment'  \n",
    "        self.rootDirectoryPathTrain = '../data/for-2seconds/training'  \n",
    "        self.rootDirectoryPathValidation = '../data/for-2seconds/validation'  \n",
    "        self.batchSize = 32\n",
    "        self.numEpochs = 2\n",
    "        self.learningRate = 3e-4\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.randomSeed = 42\n",
    "        torch.manual_seed(self.randomSeed)\n",
    "\n",
    "\n",
    "def loadModel(modelClass, stateDictPath, args):\n",
    "    model = modelClass(args, 'cpu')\n",
    "    stateDict = torch.load(stateDictPath, map_location='cpu')\n",
    "    for key in list(stateDict.keys()):\n",
    "        stateDict[key.replace('module.', '')] = stateDict.pop(key)\n",
    "    model.load_state_dict(stateDict)\n",
    "    return model\n",
    "\n",
    "def pad(x, maxLen=64600):\n",
    "    xLen = x.shape[0]\n",
    "    if xLen >= maxLen:\n",
    "        return x[:maxLen]\n",
    "    numRepeats = int(maxLen / xLen) + 1\n",
    "    paddedX = np.tile(x, (1, numRepeats))[:, :maxLen][0]\n",
    "    return paddedX\n",
    "\n",
    "def loaderRishabhSubset(samplePath):\n",
    "    cut = 64600\n",
    "    X, fs = librosa.load(samplePath, sr=16000)\n",
    "    XPad = pad(X, cut)\n",
    "    xInput = torch.Tensor(XPad)\n",
    "    return xInput\n",
    "\n",
    "def setupDataLoader(rootDirectoryPath, loaderFunc, extensions, batchSize):\n",
    "    dataset = DatasetFolder(rootDirectoryPath, loader=loaderFunc, extensions=extensions)\n",
    "    loader = DataLoader(dataset, batch_size=batchSize, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "    target_scores = np.array(target_scores)\n",
    "    nontarget_scores = np.array(nontarget_scores)\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))  # false rejection rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))  # false acceptance rates\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))  # Thresholds are the sorted scores\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "\n",
    "def compute_eer_auc(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER), AUC, and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    \n",
    "    # Compute AUC using trapezoidal rule\n",
    "    auc = np.trapz(1 - frr, far)\n",
    "\n",
    "    return eer, auc, thresholds[min_index]\n",
    "\n",
    "def evalScript(model, loader, device, experiment_save_name, savePath='B20AI056_eval', savelogs=True, printlogs=True):\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        scores = []\n",
    "        truths = []\n",
    "        for xs, labels in tqdm(loader):\n",
    "                xs, labels = xs.to(device), labels.to(device)\n",
    "                outputs = model(xs)     \n",
    "                scores.extend((outputs[:, 1]).data.cpu().numpy().ravel().tolist())\n",
    "                truths.extend(labels.tolist())\n",
    "\n",
    "    # calculate EER, AUC correctly\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(np.array(truths), np.array(scores), pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    thresh = interp1d(fpr, thresholds)(eer)\n",
    "\n",
    "    if printlogs:\n",
    "        tqdm.write('FINAL SCORES :')\n",
    "        tqdm.write('eer:', eer, '        auc:', auc,  \"    thresh:\", thresh)\n",
    "        tqdm.write()\n",
    "    \n",
    "    if savelogs:\n",
    "        with open(os.path.join(savePath, experiment_save_name), 'w') as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    'eer':    float(f'{eer}'    ) ,\n",
    "                    'auc':    float(f'{auc}'    ) ,\n",
    "                    'thresh': float(f'{thresh}' ) ,\n",
    "                }, f, indent=4\n",
    "            )\n",
    "    eer    = float(f'{eer}'    )\n",
    "    auc    = float(f'{auc}'    )\n",
    "    thresh = float(f'{thresh}' )\n",
    "    return eer, auc, thresh\n",
    "\n",
    "def trainScript(model, trainLoader, evalLoader, testLoader, config, optimizer, criterion, trainLoss=[], eerTrain=[], eerEval=[], eerTest=[], aucTrain=[], aucEval=[], aucTest=[], printlogs=True, savelogs=True):\n",
    "    model.train()\n",
    "    model = model.to(config.device)\n",
    "    optimizer = optimizer(model.parameters(), lr=config.learningRate)\n",
    "    criterion = criterion()\n",
    "\n",
    "    for epoch in range(config.numEpochs+1):\n",
    "        if epoch > 0:\n",
    "            tqdm.write(f'begun training epoch#{epoch} out of {config.numEpochs}')\n",
    "            tqdm.write(f'-' * 20)\n",
    "            bar = tqdm(total=len(trainLoader))\n",
    "            for xs, labels in trainLoader:\n",
    "                optimizer.zero_grad()\n",
    "                xs, labels = xs.to(device), labels.to(device)\n",
    "                outputs = model(xs)     \n",
    "                scores = torch.stack([outputs[:, 1], - outputs[:, 1]], dim=1)\n",
    "\n",
    "                # print(scores.shape, labels.shape)\n",
    "                loss = criterion(scores, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                trainLoss.append(loss.item())\n",
    "                bar.update(1)\n",
    "                bar.set_postfix({\n",
    "                    'trainloss(tillnow)': np.mean(np.array(trainLoss)),\n",
    "                })\n",
    "                wandb.log({\"trainloss\": loss.item()})\n",
    "            bar.close()\n",
    "        else:\n",
    "            tqdm.write('epoch 0, printing the raw results only...')\n",
    "            \n",
    "        train_auc, train_eer, train_thresh = evalScript(model, trainLoader, device, savelogs=False, printlogs=False)\n",
    "        eval_auc, eval_eer, eval_thresh = evalScript(model, evalLoader, device, savelogs=False, printlogs=False)\n",
    "        test_auc, test_eer, test_thresh = evalScript(model, testLoader, device, savelogs=False, printlogs=False)\n",
    "        aucTrain.append(train_auc )\n",
    "        eerTrain.append(train_eer )\n",
    "        aucEval.append (eval_auc  )\n",
    "        eerEval.append (eval_eer  )\n",
    "        aucTest.append (test_auc  )\n",
    "        eerTest.append (test_eer  )\n",
    "        logs = {\n",
    "            'checkpoint_save_dir': config.checkpointSaveDir,\n",
    "            'experiment_save_name': config.experimentSaveName,\n",
    "            'save_location': os.path.join(config.checkpointSaveDi, config.experimentSaveName),\n",
    "            'trainloss': trainLoss,\n",
    "            'auc_train': aucTrain,\n",
    "            'eer_train': eerTrain,\n",
    "            'auc_eval': aucEval,\n",
    "            'eer_eval': eerEval,\n",
    "            'auc_test': aucTest,\n",
    "            'eer_test': eerTest,\n",
    "            'training_epochs': epoch,\n",
    "            'learning_rate': config.learningRate,\n",
    "            'batch_size': config.batchSize,\n",
    "        }\n",
    "        wandb_log = {\n",
    "            'auc_train' : aucTrain [-1] ,\n",
    "            'eer_train' : eerTrain [-1] ,\n",
    "            'auc_eval'  : aucEval  [-1] ,\n",
    "            'eer_eval'  : eerEval  [-1] ,\n",
    "            'auc_test'  : aucTest  [-1] ,\n",
    "            'eer_test'  : eerTest  [-1] ,\n",
    "        }\n",
    "        \n",
    "        wandb.log(wandb_log)\n",
    "\n",
    "        if printlogs:\n",
    "            print(json.dumps(obj=logs, indent=4))\n",
    "\n",
    "        if savelogs:\n",
    "            torch.save({\n",
    "                'model':model.state_dict(), \n",
    "                'logs': logs\n",
    "            }, os.path.join(config.checkpointSaveDir, config.experimentSaveName + '_ckpt.pt'))\n",
    "            with open(os.path.join(config.checkpointSaveDir, config.experimentSaveName + '_logs.json'), 'w') as f:\n",
    "                json.dump(logs, f, indent=4)\n",
    "    return trainLoss, aucTrain, eerTrain, aucEval, eerEval, aucTest, eerTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    import wandb\n",
    "\n",
    "    config = Config()\n",
    "\n",
    "    wandb.init(\n",
    "        project='supa3',\n",
    "        config={\n",
    "            'learning-rate': config.learningRate,\n",
    "            'num-epochs': config.numEpochs,\n",
    "            'batch-size': config.batchSize,\n",
    "            'random-seed': config.randomSeed,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = loadModel(Model, '../models/Best_LA_model_for_DF.pth', None)\n",
    "    trainLoader = setupDataLoader(config.rootDirectoryPathTrain, loaderRishabhSubset, ('wav', 'mp3'), config.batchSize)\n",
    "    evalLoader = setupDataLoader(config.rootDirectoryPathValidation, loaderRishabhSubset, ('wav', 'mp3'), config.batchSize)\n",
    "    testLoader = setupDataLoader(config.rootDirectoryPathTesting, loaderRishabhSubset, ('wav', 'mp3'), config.batchSize)\n",
    "\n",
    "    trainLoss, aucTrain, eerTrain, aucEval, eerEval, aucTest, eerTest = [], [], [], [], [], [], []\n",
    "    trainLoss, aucTrain, eerTrain, aucEval, eerEval, aucTest, eerTest = trainScript(model, trainLoader, evalLoader, testLoader, config.numEpochs, config.learningRate, config.device, torch.optim.Adam, torch.nn.CrossEntropyLoss, trainLoss, eerTrain, eerEval, eerTest, aucTrain, aucEval, aucTest)\n",
    "\n",
    "    print('trainloss:', trainLoss[-1])\n",
    "    print('auc_train:', aucTrain[-1])\n",
    "    print('eer_train:', eerTrain[-1])\n",
    "    print('auc_eval:', aucEval[-1])\n",
    "    print('eer_eval:', eerEval[-1])\n",
    "    print('auc_test:', aucTest[-1])\n",
    "    print('eer_test:', eerTest[-1])\n",
    "\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
