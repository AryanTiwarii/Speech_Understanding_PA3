{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Understanding - Programming Assignment 3\n",
    "\n",
    "    Aryan Tiwari B20AI056\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# import gradio as gr\n",
    "import wandb as wb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "**Goal:** \n",
    "\n",
    "    The task is to classify the audio samples into Real and Fake\n",
    "\n",
    "**Tasks:** \n",
    "\n",
    "    —- Use the SSL W2V model trained for LA and DF tracks of the ASVSpoof dataset.\n",
    "\n",
    "    —- Download the custom dataset from here. Report the AUC and EER on this dataset. \n",
    "    \n",
    "    —- Analyze the performance of the model.\n",
    "    \n",
    "    —- Finetune the model on FOR dataset. \n",
    "    \n",
    "    —- Report the performance using AUC and EER on For dataset. \n",
    "    \n",
    "    —- Use the model trained on the FOR dataset to evaluate the custom dataset. Report the EER and AUC\n",
    "    \n",
    "    —- Comment on the change in performance, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TakHemlata/SSL_Anti-spoofing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import fairseq\n",
    "\n",
    "\n",
    "############################\n",
    "## FOR fine-tuned SSL MODEL\n",
    "############################\n",
    "\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(SSLModel, self).__init__()\n",
    "        \n",
    "        cp_path = 'xlsr2_300m.pt'   # Change the pre-trained XLSR model path. \n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "        self.model = model[0]\n",
    "        self.device=device\n",
    "        self.out_dim = 1024\n",
    "        return\n",
    "\n",
    "    def extract_feat(self, input_data):\n",
    "        \n",
    "        # put the model to GPU if it not there\n",
    "        if next(self.model.parameters()).device != input_data.device \\\n",
    "           or next(self.model.parameters()).dtype != input_data.dtype:\n",
    "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
    "            self.model.train()\n",
    "\n",
    "        \n",
    "        if True:\n",
    "            # input should be in shape (batch, length)\n",
    "            if input_data.ndim == 3:\n",
    "                input_tmp = input_data[:, :, 0]\n",
    "            else:\n",
    "                input_tmp = input_data\n",
    "                \n",
    "            # [batch, length, dim]\n",
    "            emb = self.model(input_tmp, mask=False, features_only=True)['x']\n",
    "        return emb\n",
    "\n",
    "\n",
    "#---------AASIST back-end------------------------#\n",
    "''' Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu and Nicholas Evans. \n",
    "    AASIST: Audio Anti-Spoofing Using Integrated Spectro-Temporal Graph Attention Networks. \n",
    "    In Proc. ICASSP 2022, pp: 6367--6371.'''\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_weight = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x   :(#bs, #node, #dim)\n",
    "        '''\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x)\n",
    "\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map(self, x):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "        att_map = torch.matmul(att_map, self.att_weight)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class HtrgGraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_type1 = nn.Linear(in_dim, in_dim)\n",
    "        self.proj_type2 = nn.Linear(in_dim, in_dim)\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_projM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.att_weight11 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight22 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight12 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weightM = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x1, x2, master=None):\n",
    "        '''\n",
    "        x1  :(#bs, #node, #dim)\n",
    "        x2  :(#bs, #node, #dim)\n",
    "        '''\n",
    "        #print('x1',x1.shape)\n",
    "        #print('x2',x2.shape)\n",
    "        num_type1 = x1.size(1)\n",
    "        num_type2 = x2.size(1)\n",
    "        #print('num_type1',num_type1)\n",
    "        #print('num_type2',num_type2)\n",
    "        x1 = self.proj_type1(x1)\n",
    "        #print('proj_type1',x1.shape)\n",
    "        x2 = self.proj_type2(x2)\n",
    "        #print('proj_type2',x2.shape)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #print('Concat x1 and x2',x.shape)\n",
    "        \n",
    "        if master is None:\n",
    "            master = torch.mean(x, dim=1, keepdim=True)\n",
    "            #print('master',master.shape)\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x, num_type1, num_type2)\n",
    "        #print('master',master.shape)\n",
    "        # directional edge for master node\n",
    "        master = self._update_master(x, master)\n",
    "        #print('master',master.shape)\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "        #print('proj x',x.shape)\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x1 = x.narrow(1, 0, num_type1)\n",
    "        #print('x1',x1.shape)\n",
    "        x2 = x.narrow(1, num_type1, num_type2)\n",
    "        #print('x2',x2.shape)\n",
    "        return x1, x2, master\n",
    "\n",
    "    def _update_master(self, x, master):\n",
    "\n",
    "        att_map = self._derive_att_map_master(x, master)\n",
    "        master = self._project_master(x, master, att_map)\n",
    "\n",
    "        return master\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map_master(self, x, master):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = x * master\n",
    "        att_map = torch.tanh(self.att_projM(att_map))\n",
    "\n",
    "        att_map = torch.matmul(att_map, self.att_weightM)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _derive_att_map(self, x, num_type1, num_type2):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "\n",
    "        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n",
    "\n",
    "        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n",
    "        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n",
    "        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n",
    "        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n",
    "\n",
    "        att_map = att_board\n",
    "\n",
    "        \n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _project_master(self, x, master, att_map):\n",
    "\n",
    "        x1 = self.proj_with_attM(torch.matmul(\n",
    "            att_map.squeeze(-1).unsqueeze(1), x))\n",
    "        x2 = self.proj_without_attM(master)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "    def forward(self, h):\n",
    "        Z = self.drop(h)\n",
    "        weights = self.proj(Z)\n",
    "        scores = self.sigmoid(weights)\n",
    "        new_h = self.top_k_graph(scores, h, self.k)\n",
    "\n",
    "        return new_h\n",
    "\n",
    "    def top_k_graph(self, scores, h, k):\n",
    "        \"\"\"\n",
    "        args\n",
    "        =====\n",
    "        scores: attention-based weights (#bs, #node, 1)\n",
    "        h: graph data (#bs, #node, #dim)\n",
    "        k: ratio of remaining nodes, (float)\n",
    "        returns\n",
    "        =====\n",
    "        h: graph pool applied data (#bs, #node', #dim)\n",
    "        \"\"\"\n",
    "        _, n_nodes, n_feat = h.size()\n",
    "        n_nodes = max(int(n_nodes * k), 1)\n",
    "        _, idx = torch.topk(scores, n_nodes, dim=1)\n",
    "        idx = idx.expand(-1, -1, n_feat)\n",
    "\n",
    "        h = h * scores\n",
    "        h = torch.gather(h, 1, idx)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, nb_filts, first=False):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "\n",
    "        if not self.first:\n",
    "            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n",
    "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(1, 1),\n",
    "                               stride=1)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
    "        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(0, 1),\n",
    "                               stride=1)\n",
    "\n",
    "        if nb_filts[0] != nb_filts[1]:\n",
    "            self.downsample = True\n",
    "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                                             out_channels=nb_filts[1],\n",
    "                                             padding=(0, 1),\n",
    "                                             kernel_size=(1, 3),\n",
    "                                             stride=1)\n",
    "\n",
    "        else:\n",
    "            self.downsample = False\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if not self.first:\n",
    "            out = self.bn1(x)\n",
    "            out = self.selu(out)\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        #print('out',out.shape)\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        #print('aft conv1 out',out.shape)\n",
    "        out = self.bn2(out)\n",
    "        out = self.selu(out)\n",
    "        # print('out',out.shape)\n",
    "        out = self.conv2(out)\n",
    "        #print('conv2 out',out.shape)\n",
    "        \n",
    "        if self.downsample:\n",
    "            identity = self.conv_downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        #out = self.mp(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # AASIST parameters\n",
    "        filts = [128, [1, 32], [32, 32], [32, 64], [64, 64]]\n",
    "        gat_dims = [64, 32]\n",
    "        pool_ratios = [0.5, 0.5, 0.5, 0.5]\n",
    "        temperatures =  [2.0, 2.0, 100.0, 100.0]\n",
    "\n",
    "\n",
    "        ####\n",
    "        # create network wav2vec 2.0\n",
    "        ####\n",
    "        self.ssl_model = SSLModel(self.device)\n",
    "        self.LL = nn.Linear(self.ssl_model.out_dim, 128)\n",
    "\n",
    "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
    "        self.first_bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.drop = nn.Dropout(0.5, inplace=True)\n",
    "        self.drop_way = nn.Dropout(0.2, inplace=True)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        # RawNet2 encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[2])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[3])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])))\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(1,1)),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 64, kernel_size=(1,1)),\n",
    "            \n",
    "        )\n",
    "        # position encoding\n",
    "        self.pos_S = nn.Parameter(torch.randn(1, 42, filts[-1][-1]))\n",
    "        \n",
    "        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "        \n",
    "        # Graph module\n",
    "        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[0])\n",
    "        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[1])\n",
    "        # HS-GAL layer \n",
    "        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "\n",
    "        # Graph pooling layers\n",
    "        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n",
    "        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n",
    "        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "\n",
    "        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        \n",
    "        self.out_layer = nn.Linear(5 * gat_dims[1], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #-------pre-trained Wav2vec model fine tunning ------------------------##\n",
    "        x_ssl_feat = self.ssl_model.extract_feat(x.squeeze(-1))\n",
    "        x = self.LL(x_ssl_feat) #(bs,frame_number,feat_out_dim)\n",
    "        \n",
    "        # post-processing on front-end features\n",
    "        x = x.transpose(1, 2)   #(bs,feat_out_dim,frame_number)\n",
    "        x = x.unsqueeze(dim=1) # add channel \n",
    "        x = F.max_pool2d(x, (3, 3))\n",
    "        x = self.first_bn(x)\n",
    "        x = self.selu(x)\n",
    "\n",
    "        # RawNet2-based encoder\n",
    "        x = self.encoder(x)\n",
    "        x = self.first_bn1(x)\n",
    "        x = self.selu(x)\n",
    "        \n",
    "        w = self.attention(x)\n",
    "        \n",
    "        #------------SA for spectral feature-------------#\n",
    "        w1 = F.softmax(w,dim=-1)\n",
    "        m = torch.sum(x * w1, dim=-1)\n",
    "        e_S = m.transpose(1, 2) + self.pos_S \n",
    "        \n",
    "        # graph module layer\n",
    "        gat_S = self.GAT_layer_S(e_S)\n",
    "        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n",
    "        \n",
    "        #------------SA for temporal feature-------------#\n",
    "        w2 = F.softmax(w,dim=-2)\n",
    "        m1 = torch.sum(x * w2, dim=-2)\n",
    "     \n",
    "        e_T = m1.transpose(1, 2)\n",
    "       \n",
    "        # graph module layer\n",
    "        gat_T = self.GAT_layer_T(e_T)\n",
    "        out_T = self.pool_T(gat_T)\n",
    "        \n",
    "        # learnable master node\n",
    "        master1 = self.master1.expand(x.size(0), -1, -1)\n",
    "        master2 = self.master2.expand(x.size(0), -1, -1)\n",
    "\n",
    "        # inference 1\n",
    "        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n",
    "            out_T, out_S, master=self.master1)\n",
    "\n",
    "        out_S1 = self.pool_hS1(out_S1)\n",
    "        out_T1 = self.pool_hT1(out_T1)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n",
    "            out_T1, out_S1, master=master1)\n",
    "        out_T1 = out_T1 + out_T_aug\n",
    "        out_S1 = out_S1 + out_S_aug\n",
    "        master1 = master1 + master_aug\n",
    "\n",
    "        # inference 2\n",
    "        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n",
    "            out_T, out_S, master=self.master2)\n",
    "        out_S2 = self.pool_hS2(out_S2)\n",
    "        out_T2 = self.pool_hT2(out_T2)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n",
    "            out_T2, out_S2, master=master2)\n",
    "        out_T2 = out_T2 + out_T_aug\n",
    "        out_S2 = out_S2 + out_S_aug\n",
    "        master2 = master2 + master_aug\n",
    "\n",
    "        out_T1 = self.drop_way(out_T1)\n",
    "        out_T2 = self.drop_way(out_T2)\n",
    "        out_S1 = self.drop_way(out_S1)\n",
    "        out_S2 = self.drop_way(out_S2)\n",
    "        master1 = self.drop_way(master1)\n",
    "        master2 = self.drop_way(master2)\n",
    "\n",
    "        out_T = torch.max(out_T1, out_T2)\n",
    "        out_S = torch.max(out_S1, out_S2)\n",
    "        master = torch.max(master1, master2)\n",
    "\n",
    "        # Readout operation\n",
    "        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n",
    "        T_avg = torch.mean(out_T, dim=1)\n",
    "\n",
    "        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n",
    "        S_avg = torch.mean(out_S, dim=1)\n",
    "        \n",
    "        last_hidden = torch.cat(\n",
    "            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n",
    "        \n",
    "        last_hidden = self.drop(last_hidden)\n",
    "        output = self.out_layer(last_hidden)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch import Tensor\n",
    "import os\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "experiment_save_name = 'custom_subset_eval_1'\n",
    "root_directory_path_rishabh_subset = '../Dataset_Speech_Assignment' \n",
    "batch_size = 8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = None\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load the model\n",
    "model = Model(args, 'cpu')\n",
    "\n",
    "# Load the best state dict .pth file\n",
    "state_dict_path = '../models/Best_LA_model_for_DF.pth'\n",
    "state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace('module.', '')] = state_dict.pop(key)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "def pad(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def loader_rishabhsubset(samplepath):\n",
    "    cut = 64600\n",
    "    X, fs = librosa.load(samplepath, sr=16000)\n",
    "    X_pad = pad(X, cut)\n",
    "    x_inp = Tensor(X_pad)\n",
    "    return x_inp\n",
    "\n",
    "\n",
    "dataset = DatasetFolder(root_directory_path_rishabh_subset, loader=loader_rishabhsubset, extensions=('wav', 'mp3', ))\n",
    "\n",
    "# Load the dataloader\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define evaluation utilities\n",
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "    target_scores = np.array(target_scores)\n",
    "    nontarget_scores = np.array(nontarget_scores)\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))\n",
    "    return frr, far, thresholds\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "def compute_eer_auc(target_scores, nontarget_scores):\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    auc = np.trapz(1 - frr, far)\n",
    "    return eer, auc, thresholds[min_index]\n",
    "\n",
    "# Evaluation script\n",
    "def eval_script(model, loader, device, savepath='B20AI056_output', printlogs=True, savelogs=True, savefigs=True):\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        scores = []\n",
    "        truths = []\n",
    "        for xs, labels in tqdm(loader):\n",
    "            xs, labels = xs.to(device), labels.to(device)\n",
    "            outputs = model(xs)\n",
    "            scores.extend((outputs[:, 1]).data.cpu().numpy().ravel().tolist())\n",
    "            truths.extend(labels.tolist())\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(np.array(truths), np.array(scores), pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        thresh = interp1d(fpr, thresholds)(eer)\n",
    "\n",
    "        if printlogs:\n",
    "            print('FINAL SCORES BELOW!!!!!!!!!!')\n",
    "            print('eer:', eer, '        auc:', auc,  \"    thresh:\", thresh)\n",
    "            print('FINAL SCORES ABOVE!!!!!!!!!!')\n",
    "            print()\n",
    "        \n",
    "        if savelogs:\n",
    "            with open(os.path.join(savepath, experiment_save_name + '.json'), 'w') as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        'eer': float(f'{eer}'),\n",
    "                        'auc': float(f'{auc}'),\n",
    "                        'thresh': float(f'{thresh}'),\n",
    "                    }, f, indent=4\n",
    "                )\n",
    "            \n",
    "        if savefigs:    \n",
    "            # Plot ROC curve and save\n",
    "            image_save_path = os.path.join(savepath, experiment_save_name + '.png')\n",
    "            plt.figure()\n",
    "            lw = 2\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % auc)\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(image_save_path)\n",
    "\n",
    "    return eer, auc\n",
    "\n",
    "eval_script(model, loader, device, printlogs=False, savelogs=False, savefigs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch import Tensor\n",
    "import os\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from model import Model\n",
    "\n",
    "###########################################\n",
    "checkpoint_save_dir = 'checkpoint_save_dir'\n",
    "experiment_save_name = 'train_eval_2_final'\n",
    "\n",
    "# root_directory_path_rishabh_subset = '../Dataset_Speech_Assignment' # contains .wav and .mp3 files\n",
    "\n",
    "root_directory_path_testing = '../Dataset_Speech_Assignment'  # contains .wav files only.\n",
    "# root_directory_path_testing = '../for-2seconds/testing'  # contains .wav files only.\n",
    "root_directory_path_train = '../for-2seconds/training'  # contains .wav files only.\n",
    "root_directory_path_validation = '../for-2seconds/validation'  # contains .wav files only.\n",
    "\n",
    "batch_size = 32\n",
    "nepochs = 2\n",
    "learningrate = 3e-4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = None\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "###########################################\n",
    "# wandb\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project='supa3',\n",
    "    \n",
    "    config={\n",
    "        'learning-rate': learningrate,\n",
    "        'num-epochs': nepochs,\n",
    "        'batch-size': batch_size,\n",
    "        'random-seed': random_seed,\n",
    "    }\n",
    ")\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = Model(args, 'cpu')\n",
    "\n",
    "# load the best state dict .pth file\n",
    "state_dict_path = '../models/Best_LA_model_for_DF.pth'\n",
    "state_dict = torch.load(state_dict_path, map_location='cpu')\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace('module.', '')] = state_dict.pop(key)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "def pad(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = int(max_len / x_len)+1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def loader_rishabhsubset(samplepath):\n",
    "    cut = 64600\n",
    "    X, fs = librosa.load(samplepath, sr=16000)\n",
    "    X_pad = pad(X, cut)\n",
    "    x_inp = Tensor(X_pad)\n",
    "    return x_inp\n",
    "\n",
    "\n",
    "train_dataset = DatasetFolder(root_directory_path_train, loader=loader_rishabhsubset, extensions=('wav', 'mp3', ))\n",
    "eval_dataset  = DatasetFolder(root_directory_path_validation, loader=loader_rishabhsubset, extensions=('wav', 'mp3', ))\n",
    "test_dataset  = DatasetFolder(root_directory_path_testing, loader=loader_rishabhsubset, extensions=('wav', 'mp3', ))\n",
    "\n",
    "# load the dataloader. \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader  = DataLoader(eval_dataset , batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset , batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#define the eval utils\n",
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "    target_scores = np.array(target_scores)\n",
    "    nontarget_scores = np.array(nontarget_scores)\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))  # false rejection rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))  # false acceptance rates\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))  # Thresholds are the sorted scores\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "\n",
    "def compute_eer_auc(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER), AUC, and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    \n",
    "    # Compute AUC using trapezoidal rule\n",
    "    auc = np.trapz(1 - frr, far)\n",
    "\n",
    "    return eer, auc, thresholds[min_index]\n",
    "\n",
    "\n",
    "# define the eval script. it should output two lists:\n",
    "# # 1. list_bonafide should contain the pred scores for REAL audios\n",
    "# # 2. list_spoof should contain the pred scores for FAKE audios\n",
    "def eval_script(model, loader, device, savepath='B20AI056_eval', savelogs=True, printlogs=True):\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        scores = []\n",
    "        truths = []\n",
    "        for xs, labels in tqdm(loader):\n",
    "                xs, labels = xs.to(device), labels.to(device)\n",
    "                outputs = model(xs)     \n",
    "                scores.extend((outputs[:, 1]).data.cpu().numpy().ravel().tolist())\n",
    "                truths.extend(labels.tolist())\n",
    "    \n",
    "    # #calculate EER, AUC\n",
    "    # target_scores = [score for score, truth in zip(scores, truths) if truth == 0]\n",
    "    # nontarget_scores = [score for score, truth in zip(scores, truths) if truth == 1]\n",
    "    # eer_cm, _, _ = compute_eer_auc(target_scores, nontarget_scores)\n",
    "\n",
    "    # calculate EER, AUC correctly\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(np.array(truths), np.array(scores), pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    thresh = interp1d(fpr, thresholds)(eer)\n",
    "\n",
    "    if printlogs:\n",
    "        tqdm.write('FINAL SCORES BELOW!!!!!!!!!!')\n",
    "        tqdm.write('eer:', eer, '        auc:', auc,  \"    thresh:\", thresh)\n",
    "        tqdm.write('FINAL SCORES ABOVE!!!!!!!!!!')\n",
    "        tqdm.write()\n",
    "    \n",
    "    if savelogs:\n",
    "        with open(os.path.join(savepath, experiment_save_name), 'w') as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    'eer':    float(f'{eer}'    ) ,\n",
    "                    'auc':    float(f'{auc}'    ) ,\n",
    "                    'thresh': float(f'{thresh}' ) ,\n",
    "                }, f, indent=4\n",
    "            )\n",
    "    eer    = float(f'{eer}'    )\n",
    "    auc    = float(f'{auc}'    )\n",
    "    thresh = float(f'{thresh}' )\n",
    "    return eer, auc, thresh\n",
    "\n",
    "\n",
    "# eval_script(model, loader, device)\n",
    "\n",
    "# after this, start doing shit inside train.py. figure out the training and then train for 1 epoch and then run eval straight after the training and recompute everything.\n",
    "\n",
    "def train_script(model, train_loader, eval_loader, test_loader, num_epochs, learning_rate, device, optimizer, criterion, trainloss=[], eer_train=[], eer_eval=[], eer_test=[], auc_train=[], auc_eval=[], auc_test=[], printlogs=True, savelogs=True):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = criterion()\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        if epoch > 0:\n",
    "            tqdm.write(f'begun training epoch#{epoch} out of {num_epochs}')\n",
    "            tqdm.write(f'-' * 20)\n",
    "            bar = tqdm(total=len(train_loader))\n",
    "            for xs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                xs, labels = xs.to(device), labels.to(device)\n",
    "                outputs = model(xs)     \n",
    "                scores = torch.stack([outputs[:, 1], - outputs[:, 1]], dim=1)\n",
    "\n",
    "                # print(scores.shape, labels.shape)\n",
    "                loss = criterion(scores, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                trainloss.append(loss.item())\n",
    "                bar.update(1)\n",
    "                bar.set_postfix({\n",
    "                    'trainloss(tillnow)': np.mean(np.array(trainloss)),\n",
    "                })\n",
    "                wandb.log({\"trainloss\": loss.item()})\n",
    "            bar.close()\n",
    "        else:\n",
    "            tqdm.write('epoch 0, printing the raw results only...')\n",
    "            \n",
    "        train_auc, train_eer, train_thresh = eval_script(model, train_loader, device, savelogs=False, printlogs=False)\n",
    "        eval_auc, eval_eer, eval_thresh = eval_script(model, eval_loader, device, savelogs=False, printlogs=False)\n",
    "        test_auc, test_eer, test_thresh = eval_script(model, test_loader, device, savelogs=False, printlogs=False)\n",
    "        auc_train.append(train_auc )\n",
    "        eer_train.append(train_eer )\n",
    "        auc_eval.append (eval_auc  )\n",
    "        eer_eval.append (eval_eer  )\n",
    "        auc_test.append (test_auc  )\n",
    "        eer_test.append (test_eer  )\n",
    "        logs = {\n",
    "            'checkpoint_save_dir': checkpoint_save_dir,\n",
    "            'experiment_save_name': experiment_save_name,\n",
    "            'save_location': os.path.join(checkpoint_save_dir, experiment_save_name),\n",
    "            'trainloss': trainloss,\n",
    "            'auc_train': auc_train,\n",
    "            'eer_train': eer_train,\n",
    "            'auc_eval': auc_eval,\n",
    "            'eer_eval': eer_eval,\n",
    "            'auc_test': auc_test,\n",
    "            'eer_test': eer_test,\n",
    "            'training_epochs': epoch,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "        }\n",
    "        \n",
    "        wandb_log = {\n",
    "            'auc_train' : auc_train [-1] ,\n",
    "            'eer_train' : eer_train [-1] ,\n",
    "            'auc_eval'  : auc_eval  [-1] ,\n",
    "            'eer_eval'  : eer_eval  [-1] ,\n",
    "            'auc_test'  : auc_test  [-1] ,\n",
    "            'eer_test'  : eer_test  [-1] ,\n",
    "        }\n",
    "        \n",
    "        wandb.log(wandb_log)\n",
    "\n",
    "        if printlogs:\n",
    "            print(json.dumps(obj=logs, indent=4))\n",
    "\n",
    "        if savelogs:\n",
    "            torch.save({\n",
    "                'model':model.state_dict(), \n",
    "                'logs': logs\n",
    "            }, os.path.join(checkpoint_save_dir, experiment_save_name + '_ckpt.pt'))\n",
    "            with open(os.path.join(checkpoint_save_dir, experiment_save_name + '_logs.json'), 'w') as f:\n",
    "                json.dump(logs, f, indent=4)\n",
    "    return trainloss, auc_train, eer_train, auc_eval, eer_eval, auc_test, eer_test\n",
    "\n",
    "trainloss, auc_train, eer_train, auc_eval, eer_eval, auc_test, eer_test = [], [], [], [], [], [], []\n",
    "trainloss, auc_train, eer_train, auc_eval, eer_eval, auc_test, eer_test = train_script(model, train_loader, eval_loader, test_loader, nepochs, learningrate, device, torch.optim.Adam, torch.nn.CrossEntropyLoss, trainloss, eer_train, eer_eval, eer_test, auc_train, auc_eval, auc_test)\n",
    "\n",
    "print('trainloss: ', trainloss[-1])\n",
    "print('auc_train: ', auc_train[-1])\n",
    "print('eer_train: ', eer_train[-1])\n",
    "print('auc_eval: ', auc_eval[-1])\n",
    "print('eer_eval: ', eer_eval[-1])\n",
    "print('auc_test: ', auc_test[-1])\n",
    "print('eer_test: ', eer_test[-1])\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# threshold = -3.4\n",
    "# preds = [int(score > threshold) for score in scores]\n",
    "# total = sum([int(pred == pred) for (pred, truth) in zip(preds, truths)])\n",
    "# correct = sum([int(pred == truth) for (pred, truth) in zip(preds, truths)])\n",
    "# total, correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
